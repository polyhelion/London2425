{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNy3ymgM44xi+R0qq2jj4M8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Single-head scaled dot product attention (SDPA) using the CUTLASS GEMM  API\n","\n","Simple sandbox example (single batch, single head, row-major). Significant potential for optimization <br>\n","-> Tiling (FlashAttention, ThunderKittens, ...).\n","\n","Input sequences $\\, Q, K, V \\in \\mathbb{R}^{N \\times d_k}$ where $N$ is the sequence length and $d_k$ is key (and query) vector dimension per head. \\\\\n","\n","GEMM 1 : $S = Q K^\\top \\in \\mathbb{R}^{N \\times N}$ <br>\n","Scaling + Row-softmax : $P = \\mathrm{softmax}\\,\\left(\\, S / \\sqrt{d_k} \\, \\right) \\in \\mathbb{R}^{N \\times N} $ <br>\n","GEMM 2 : $O = P\\,V \\in \\mathbb{R}^{N \\times d_k}$\n","\n"],"metadata":{"id":"fvaM8WyWlfVO"}},{"cell_type":"code","source":["!nvcc --version\n","!nvidia-smi --query-gpu=compute_cap --format=csv,noheader | awk -F. '{printf \"\\nCompute capability : sm_%d%d\\n\",$1,$2}'"],"metadata":{"id":"aQ33mGalhJei"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kyWMX9NdDWD"},"outputs":[],"source":["%pip install nvcc4jupyter"]},{"cell_type":"code","source":["%load_ext nvcc4jupyter"],"metadata":{"id":"m4SHt1B8dwRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/NVIDIA/cutlass.git\n","%env CUTLASS_PATH=/content/cutlass\n","%env CUTLASS_INCLUDE=/content/cutlass/include"],"metadata":{"id":"2egc5iy5165r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%cuda_group_save --name \"sdpa.cu\" --group \"sdpa\"\n","\n","#include <cmath>\n","#include <cfloat>\n","#include <cstdio>\n","#include <vector>\n","#include <random>\n","#include <limits>\n","\n","#include <cuda_runtime.h>\n","#include <math_constants.h>\n","#include <cutlass/gemm/device/gemm.h>\n","#include <cutlass/layout/matrix.h>\n","#include <cutlass/numeric_types.h>\n","#include <cutlass/array.h>\n","\n","\n","inline cudaError_t CUDA_CHECK(cudaError_t result) {\n","    if (result != cudaSuccess) {\n","      std::fprintf(stderr, \"CUDA error at %s:%d -> %s\\n\", __FILE__, __LINE__, \\\n","              cudaGetErrorString(result) );\n","      std::exit(EXIT_FAILURE);\n","    }\n","    return result;\n","}\n","\n","template <typename T> struct ComputeType { using type = T; };\n","template <> struct ComputeType<float> { using type = float; };\n","template <> struct ComputeType<double> { using type = double; };\n","template <> struct ComputeType<cutlass::half_t> { using type = float; };\n","template <> struct ComputeType<cutlass::bfloat16_t> { using type = float; };\n","\n","template <typename T> __device__ inline T neg_inf() {}\n","\n","template <> __device__ inline float neg_inf<float>()\n","{ return -CUDART_INF_F; }\n","\n","template <> __device__ inline double neg_inf<double>()\n","{ return -CUDART_INF; };\n","\n","template <> __device__ inline cutlass::half_t\n","  neg_inf<cutlass::half_t>() { return cutlass::half_t(-65504.0f); }\n","\n","template <> __device__ inline cutlass::bfloat16_t\n","  neg_inf<cutlass::bfloat16_t>() {\n","      return cutlass::bfloat16_t(-3.38953139e38f); }\n","\n","\n","\n","\n","// Naive device transpose for row-major matrices\n","template <typename T>\n","__global__ void transpose_rowmajor(const T* __restrict__ in,\n","                                         T* __restrict__ out,\n","                                         int rows_in,\n","                                         int cols_in) {\n","\n","  int r = blockIdx.y * blockDim.y + threadIdx.y;\n","  int c = blockIdx.x * blockDim.x + threadIdx.x;\n","  if (r < rows_in && c < cols_in) {\n","    // in[r, c] -> out[c, r]\n","    out[c * rows_in + r] = in[r * cols_in + c];\n","  }\n","\n","}\n","\n","\n","// Naive device per-row softmax: one thread handles one row\n","// (ok for small N)\n","template <typename T>\n","__global__ void row_softmax(T* __restrict__ scores,\n","                            int N,\n","                            float inv_sqrt_dk) {\n","\n","  using ComputeT = typename ComputeType<T>::type;\n","\n","  int row = blockIdx.x * blockDim.x + threadIdx.x;\n","  if (row >= N) return;\n","\n","  // Scale, then compute max for numerical stability\n","  // Note that softmax(x) = softmax(x + c), c a constant\n","  // We use c = max(x)\n","  ComputeT max_val = neg_inf<ComputeT>();\n","  T* row_ptr = scores + row * N;\n","  for (int j = 0; j < N; ++j) {\n","    ComputeT row_val =\n","      static_cast<ComputeT>(row_ptr[j]) * static_cast<ComputeT>(inv_sqrt_dk);\n","    row_ptr[j] = static_cast<T>(row_val);\n","    if (row_val > max_val) max_val = row_val;\n","  }\n","\n","  // Exponentiate and sum\n","  ComputeT sum = static_cast<ComputeT>(0.0);\n","  for (int j = 0; j < N; ++j) {\n","    ComputeT row_val = static_cast<ComputeT>(row_ptr[j]);\n","    sum += std::exp(row_val - max_val);\n","  }\n","\n","  // Normalize\n","  for (int j = 0; j < N; ++j) {\n","    ComputeT row_val = static_cast<ComputeT>(row_ptr[j]);\n","    ComputeT softmax_val = std::exp(row_val - max_val) / sum;\n","    row_ptr[j] = static_cast<T>(softmax_val);\n","  }\n","\n","}\n","\n","\n","int main() {\n","\n","  // Problem sizes (single batch, single head)\n","  constexpr int N = 128;   // sequence length\n","  constexpr int D = 64;    // d_k\n","\n","  // row major matrix layout everywhere\n","  //using Element = float;\n","  using Element = cutlass::half_t;\n","  constexpr auto soEl = sizeof(Element);\n","\n","  const float inv_sqrt_dk = 1.0f / std::sqrt((float)D);\n","\n","\n","  using Layout = cutlass::layout::RowMajor;\n","\n","  using Architecture = cutlass::arch::Sm75; // for Google Colab\n","\n","  // Host buffers (row-major)\n","  std::vector<Element> hQ(N*D), hK(N*D), hV(N*D);\n","\n","  // Init with random values\n","  std::random_device rd;\n","  std::mt19937 rng(rd());\n","  std::uniform_real_distribution<float> dist(-0.1f, 0.1f);\n","  for (auto* arr : {&hQ, &hK, &hV}) {\n","    for (auto& x : *arr) {\n","        x = static_cast<Element>(dist(rng));\n","    }\n","  }\n","\n","  // Device buffers\n","  Element *dQ = nullptr, *dK = nullptr, *dKt = nullptr, *dV = nullptr;\n","  Element *dScores = nullptr, *dOut = nullptr;\n","\n","  CUDA_CHECK(cudaMalloc(&dQ,      N * D * soEl));\n","  CUDA_CHECK(cudaMalloc(&dK,      N * D * soEl));\n","  CUDA_CHECK(cudaMalloc(&dKt,     D * N * soEl));  // K^T\n","  CUDA_CHECK(cudaMalloc(&dV,      N * D * soEl));\n","  CUDA_CHECK(cudaMalloc(&dScores, N * N * soEl));  // Q K^T\n","  CUDA_CHECK(cudaMalloc(&dOut,    N * D * soEl));\n","\n","  CUDA_CHECK(cudaMemcpy(dQ, hQ.data(), N * D * soEl, cudaMemcpyHostToDevice));\n","  CUDA_CHECK(cudaMemcpy(dK, hK.data(), N * D * soEl, cudaMemcpyHostToDevice));\n","  CUDA_CHECK(cudaMemcpy(dV, hV.data(), N * D * soEl, cudaMemcpyHostToDevice));\n","\n","  // Compute K^T on device (row-major transpose)\n","  dim3 blockT(16, 16);\n","  dim3 gridT((D + blockT.x - 1) / blockT.x, (N + blockT.y - 1) / blockT.y);\n","\n","  transpose_rowmajor<<<gridT, blockT>>>(dK, dKt, /*rows_in=*/N, /*cols_in=*/D);\n","  CUDA_CHECK(cudaGetLastError());\n","\n","  // CUTLASS GEMM definitions\n","  // Gemm computes: D = alpha * A * B + beta * C\n","  using Gemm = cutlass::gemm::device::Gemm<\n","      Element, Layout,   // A\n","      Element, Layout,   // B\n","      Element, Layout    // C/D\n","  //    float,\n","  //    Architecture\n","  >;\n","\n","  // GEMM 1: Scores = Q [N x D] * K^T [D x N] -> [N x N]\n","  {\n","\n","    Element alpha = static_cast<Element>(1.0);\n","    Element beta  = static_cast<Element>(0.0);\n","\n","    int lda = D;  // row-major leading dimension = columns\n","    int ldb = N;\n","    int ldc = N;\n","\n","    Gemm gemm_op;\n","\n","    Gemm::Arguments args(\n","      {N, N, D},       // GEMM problem dimensions\n","      {dQ,  lda},      // A\n","      {dKt, ldb},      // B\n","      {dScores, ldc},  // C\n","      {dScores, ldc},  // D (in-place OK when beta=0)\n","      {alpha, beta}\n","    );\n","\n","    auto status = gemm_op(args);\n","\n","    if (status != cutlass::Status::kSuccess) {\n","      std::fprintf(stderr, \"GEMM 1 launch failed: %d\\n\", int(status));\n","      return EXIT_FAILURE;\n","    }\n","  }\n","\n","\n","  // Row-wise softmax with scaling by 1/sqrt(d_k)\n","  {\n","    int threads = 128;\n","    int blocks = (N + threads - 1) / threads;\n","    row_softmax<<<blocks, threads>>>(dScores, N, inv_sqrt_dk);\n","    CUDA_CHECK(cudaGetLastError());\n","  }\n","\n","\n","  // GEMM 2: Out = P [N x N] * V [N x D] -> [N x D]\n","  {\n","\n","    Element alpha = static_cast<Element>(1.0);\n","    Element beta  = static_cast<Element>(0.0);\n","\n","    int lda = N;  // P leading dimension (columns)\n","    int ldb = D;  // V leading dimension (columns)\n","    int ldc = D;  // Out leading dimension (columns)\n","\n","    Gemm gemm_op;\n","\n","    Gemm::Arguments args(\n","      {N, D, N},       // GEMM problem dimensions\n","      {dScores, lda},  // A = P\n","      {dV,      ldb},  // B = V\n","      {dOut,    ldc},  // C\n","      {dOut,    ldc},  // D\n","      {alpha, beta}\n","    );\n","\n","    auto status = gemm_op(args);\n","\n","    if (status != cutlass::Status::kSuccess) {\n","      std::fprintf(stderr, \"GEMM 2 launch failed: %d\\n\", int(status));\n","      return EXIT_FAILURE;\n","    }\n","  }\n","\n","  CUDA_CHECK(cudaDeviceSynchronize());\n","\n","\n","  // Copy back result\n","  std::vector<Element> hOut(N*D);\n","  CUDA_CHECK(cudaMemcpy(hOut.data(), dOut, N * D * soEl, \\\n","                        cudaMemcpyDeviceToHost));\n","\n","  std::printf(\"O[0, 0..9]: \");\n","  for (int j = 0; j < 9 && j < D; ++j) std::printf(\"%.6f \", \\\n","                        static_cast<float>(hOut[j]));\n","  std::printf(\"\\n\");\n","\n","\n","  // Cleanup\n","  cudaFree(dQ); cudaFree(dK); cudaFree(dKt); cudaFree(dV);\n","  cudaFree(dScores); cudaFree(dOut);\n","\n","  return EXIT_SUCCESS;\n","}\n"],"metadata":{"id":"S9js2M-Wd5SC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cuda_group_run --group \"sdpa\" --compiler-args \"--include-path /content/cutlass/include --gpu-architecture sm_75\""],"metadata":{"id":"0PizsJwxDAb2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To build directly with nvcc, localise source file folder of nvcc4jupyter (see nvcc4jupyter ouput above)"],"metadata":{"id":"cq1wfkwJGCVa"}},{"cell_type":"code","source":["!ls -al /tmp/tmpp7f7xysa/sdpa"],"metadata":{"id":"Mu9Lhw4tcXd4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -O3 -arch=sm_75 -I$CUTLASS_INCLUDE -o sdpa sdpa.cu"],"metadata":{"id":"1mZNAodvzWDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./sdpa"],"metadata":{"id":"EZO0HpquRRta"},"execution_count":null,"outputs":[]}]}